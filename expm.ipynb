{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f80122c",
   "metadata": {},
   "source": [
    "# Extract entities and triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from dotenv import dotenv_values\n",
    "import requests\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "class HuggingFaceWrapper:\n",
    "    def __init__(self, model_id, api_key=None, temperature=0.9, max_tokens=512):\n",
    "        self.model_id = model_id\n",
    "        self.api_key = api_key if api_key else config[\"HF_API_KEY\"]  # API key from .env file or parameter\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        try:\n",
    "            payload = {\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"max_new_tokens\": self.max_tokens,\n",
    "                    \"return_full_text\": False  # Only return the generated text, not the prompt\n",
    "                }\n",
    "            }\n",
    "            response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "            return response.json()[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Hugging Face API: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "#model for extraction\n",
    "model_id = 'meta-llama/Llama-3-70b-chat-hf'  # Example model ID\n",
    "llm = HuggingFaceWrapper(model_id)\n",
    "\n",
    "\n",
    "# NER instructions\n",
    "ner_instruction = (\n",
    "    \"Your task is to extract named entities from the given paragraph.\\n\"\n",
    "    \"Focus on people, organizations, locations, dates, and other proper nouns.\\n\"\n",
    "    \"Respond with a JSON object with a key 'named_entities' whose value is a list of entities.\\n\"\n",
    "    \"If there are no entities, return {\\\"named_entities\\\": []}.\\n\"\n",
    "    \"Output only the JSON.\"\n",
    ")\n",
    "\n",
    "\n",
    "ner_example_passage_1 = (\n",
    "    \"Apple Inc. announced its new iPhone 14 series on September 7, 2022, at the Steve Jobs Theater.\\n\"\n",
    "    \"CEO Tim Cook led the presentation where he also introduced Apple Watch Series 8.\\n\"\n",
    "    \"The company expects to ship these devices across the United States and Europe starting next week.\"\n",
    ")\n",
    "\n",
    "ner_example_entities_1 = (\n",
    "    \"{\\\"named_entities\\\": [\\\"Apple Inc.\\\", \\\"iPhone 14\\\", \\\"September 7, 2022\\\", \\\"Steve Jobs Theater\\\", \\\"Tim Cook\\\", \\\"Apple Watch Series 8\\\", \\\"United States\\\", \\\"Europe\\\"]}\"\n",
    ")\n",
    "\n",
    "#prompt template for NER\n",
    "ner_prompts = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(ner_instruction),\n",
    "    HumanMessage(f\"Paragraph:\\n{ner_example_passage_1}\\n\"),\n",
    "    AIMessage(ner_example_entities_1),\n",
    "    HumanMessagePromptTemplate.from_template(\"Paragraph:\\n```\\n{user_input}\\n```\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "openie_post_ner_instruction = (\n",
    "    \"Your task is to extract relationship triples from the given paragraph using the named entities provided.\\n\"\n",
    "    \"Each triple should be in the form [subject, relation, object] where subject and object are preferably named entities.\\n\"\n",
    "    \"Respond with a JSON object with a key 'triples' whose value is a list of triples.\\n\"\n",
    "    \"Make sure to resolve pronouns to their explicit references.\\n\"\n",
    "    \"Output only the JSON.\"\n",
    ")\n",
    "\n",
    "openie_example_input_1 = (\n",
    "    f\"Convert the following passage into RDF triples using the named entities.\\n\"\n",
    "    f\"Paragraph:\\n{ner_example_passage_1}\\n\\n{ner_example_entities_1}\"\n",
    ")\n",
    "\n",
    "openie_example_triples_1 = (\n",
    "    \"{\\\"triples\\\": [\\n\"\n",
    "    \"    [\\\"Apple Inc.\\\", \\\"announced\\\", \\\"iPhone 14\\\"],\\n\"\n",
    "    \"    [\\\"Apple Inc.\\\", \\\"announced on\\\", \\\"September 7, 2022\\\"],\\n\"\n",
    "    \"    [\\\"Apple Inc.\\\", \\\"held event at\\\", \\\"Steve Jobs Theater\\\"],\\n\"\n",
    "    \"    [\\\"Tim Cook\\\", \\\"is CEO of\\\", \\\"Apple Inc.\\\"],\\n\"\n",
    "    \"    [\\\"Tim Cook\\\", \\\"introduced\\\", \\\"Apple Watch Series 8\\\"],\\n\"\n",
    "    \"    [\\\"Apple Inc.\\\", \\\"will ship to\\\", \\\"United States\\\"],\\n\"\n",
    "    \"    [\\\"Apple Inc.\\\", \\\"will ship to\\\", \\\"Europe\\\"]\\n\"\n",
    "    \"]}\"\n",
    ")\n",
    "\n",
    "\n",
    "openie_post_ner_prompts = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(openie_post_ner_instruction),\n",
    "    HumanMessage(openie_example_input_1),\n",
    "    AIMessage(openie_example_triples_1),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Convert the following passage into RDF triples using the named entities.\\n\"\n",
    "        \"Paragraph:\\n{passage}\\n\\n{named_entity_json}\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def parse_json_response(response_text, default_key):\n",
    "    \"\"\"\n",
    "    Remove extra formatting and extract all JSON objects from the response.\n",
    "    Return the one that contains the expected key.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if response_text is None:\n",
    "            return []\n",
    "        \n",
    "        # Clean up the response text to handle markdown formatting\n",
    "        cleaned = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        # Try to find JSON objects in the text\n",
    "        json_matches = re.findall(r'({.*?})', cleaned, re.DOTALL)\n",
    "        \n",
    "        for match in json_matches:\n",
    "            try:\n",
    "                parsed = json.loads(match)\n",
    "                if isinstance(parsed, dict) and default_key in parsed:\n",
    "                    return parsed[default_key]\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # Try parsing the whole cleaned response\n",
    "        try:\n",
    "            parsed = json.loads(cleaned)\n",
    "            if isinstance(parsed, dict):\n",
    "                return parsed.get(default_key, [])\n",
    "            return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # If parsing fails, return an empty list\n",
    "        return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Processing Functions\n",
    "# ---------------------------\n",
    "def process_text(text):\n",
    "    # --- NER Step ---\n",
    "    final_ner_prompt = ner_prompts.format(user_input=text)\n",
    "    ner_response = llm(final_ner_prompt)\n",
    "    print(\"NER Response received\")\n",
    "    named_entities = parse_json_response(ner_response, \"named_entities\")\n",
    "    if named_entities is None:\n",
    "        named_entities = []\n",
    "    \n",
    "    # --- OpenIE Step ---\n",
    "    final_openie_prompt = openie_post_ner_prompts.format(\n",
    "        passage=text,\n",
    "        named_entity_json=json.dumps({\"named_entities\": named_entities})\n",
    "    )\n",
    "    \n",
    "    openie_response = llm(final_openie_prompt)\n",
    "    print(\"OpenIE Response received\")\n",
    "    triples = parse_json_response(openie_response, \"triples\")\n",
    "    if triples is None:\n",
    "        triples = []\n",
    "    \n",
    "    return {\n",
    "        \"extracted_entities\": named_entities,\n",
    "        \"extracted_triples\": triples\n",
    "    }\n",
    "\n",
    "def process_jsonl_file(input_file, output_file):\n",
    "    results = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in tqdm(lines, desc=\"Processing documents\"):\n",
    "        try:\n",
    "            entry = json.loads(line.strip())\n",
    "            text = entry.get(\"text\") or entry.get(\"passage\")\n",
    "            title = entry.get(\"title\", \"\")\n",
    "            if text:\n",
    "                processed = process_text(text)\n",
    "                result = {\n",
    "                    \"title\": title,\n",
    "                    \"passage\": text,\n",
    "                    \"extracted_entities\": processed[\"extracted_entities\"],\n",
    "                    \"extracted_triples\": processed[\"extracted_triples\"]\n",
    "                }\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line: {e}\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"filtered_output.jsonl\"  # jsonl file {\"title\" : .... , \"text\": ......} \n",
    "    output_file = \"extraction_results.json\"   \n",
    "    results = process_jsonl_file(input_file, output_file)\n",
    "    print(f\"Processed {len(results)} documents. Results saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce63a3",
   "metadata": {},
   "source": [
    "# Knowledge graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_array\n",
    "from processing import *\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "import copy\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'FALSE'\n",
    "\n",
    "version = 'v3'\n",
    "inter_triple_weight = 1.0\n",
    "similarity_max = 1.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"corpus.json\" #corpus file\n",
    "extracted_file = json.load(open(file_path))\n",
    "\n",
    "cosine_sim_edges = False\n",
    "\n",
    "extracted_triples = extracted_file['docs']\n",
    "\n",
    "\n",
    "phrase_type = 'ents_only_lower_preprocess'  \n",
    "if cosine_sim_edges:\n",
    "    graph_type = 'facts_and_sim'  \n",
    "else:\n",
    "    graph_type = 'facts'\n",
    "\n",
    "passage_json = []\n",
    "phrases = []\n",
    "entities = []\n",
    "relations = {}\n",
    "incorrectly_formatted_triples = []\n",
    "triples_wo_ner_entity = []\n",
    "triple_tuples = []\n",
    "full_neighborhoods = {}\n",
    "correct_wiki_format = 0\n",
    "\n",
    "for i, row in tqdm(enumerate(extracted_triples), total=len(extracted_triples)):\n",
    "    document = row['passage']\n",
    "    raw_ner_entities = row['extracted_entities']\n",
    "    ner_entities = [processing_phrases(p) for p in row['extracted_entities']]\n",
    "\n",
    "    triples = row['extracted_triples']\n",
    "\n",
    "    doc_json = row\n",
    "\n",
    "    clean_triples = []\n",
    "    unclean_triples = []\n",
    "    doc_entities = set()\n",
    "\n",
    "\n",
    "    for triple in triples:\n",
    "\n",
    "        triple = [str(s) for s in triple]\n",
    "\n",
    "        if len(triple) > 1:\n",
    "            if len(triple) != 3:\n",
    "                clean_triple = [processing_phrases(p) for p in triple]\n",
    "\n",
    "                incorrectly_formatted_triples.append(triple)\n",
    "                unclean_triples.append(triple)\n",
    "            else:\n",
    "                clean_triple = [processing_phrases(p) for p in triple]\n",
    "\n",
    "                clean_triples.append(clean_triple)\n",
    "                phrases.extend(clean_triple)\n",
    "\n",
    "                head_ent = clean_triple[0]\n",
    "                tail_ent = clean_triple[2]\n",
    "\n",
    "                if head_ent not in ner_entities and tail_ent not in ner_entities:\n",
    "                    triples_wo_ner_entity.append(triple)\n",
    "\n",
    "                relations[(head_ent, tail_ent)] = clean_triple[1]\n",
    "\n",
    "                raw_head_ent = triple[0]\n",
    "                raw_tail_ent = triple[2]\n",
    "\n",
    "                entity_neighborhood = full_neighborhoods.get(raw_head_ent, set())\n",
    "                entity_neighborhood.add((raw_head_ent, triple[1], raw_tail_ent))\n",
    "                full_neighborhoods[raw_head_ent] = entity_neighborhood\n",
    "\n",
    "                entity_neighborhood = full_neighborhoods.get(raw_tail_ent, set())\n",
    "                entity_neighborhood.add((raw_head_ent, triple[1], raw_tail_ent))\n",
    "                full_neighborhoods[raw_tail_ent] = entity_neighborhood\n",
    "\n",
    "                for triple_entity in [clean_triple[0], clean_triple[2]]:\n",
    "                    entities.append(triple_entity)\n",
    "                    doc_entities.add(triple_entity)\n",
    "\n",
    "    doc_json['entities'] = list(set(doc_entities))\n",
    "    doc_json['clean_triples'] = clean_triples\n",
    "    doc_json['noisy_triples'] = unclean_triples\n",
    "    triple_tuples.append(clean_triples)\n",
    "\n",
    "\n",
    "unique_phrases = list(np.unique(entities))\n",
    "unique_relations = np.unique(list(relations.values()) + ['equivalent'])\n",
    "\n",
    "all_phrases = copy.deepcopy(unique_phrases)\n",
    "\n",
    "kb = pd.DataFrame(unique_phrases, columns=['strings'])\n",
    "kb2 = copy.deepcopy(kb)\n",
    "kb['type'] = 'query'\n",
    "kb2['type'] = 'kb'\n",
    "kb_full = pd.concat([kb, kb2])\n",
    "\n",
    "rel_kb = pd.DataFrame(unique_relations, columns=['strings'])\n",
    "rel_kb2 = copy.deepcopy(rel_kb)\n",
    "rel_kb['type'] = 'query'\n",
    "rel_kb2['type'] = 'kb'\n",
    "rel_kb_full = pd.concat([rel_kb, rel_kb2])\n",
    "\n",
    "\n",
    "\n",
    "kb['type'] = 'kb'\n",
    "\n",
    "create_graph_flag = True\n",
    "if create_graph_flag:\n",
    "    print('Creating Graph')\n",
    "\n",
    "    node_json = [{'idx': i, 'name': p} for i, p in enumerate(unique_phrases)]\n",
    "    kb_phrase_df = pd.DataFrame(unique_phrases)\n",
    "    kb_phrase_dict = {p: i for i, p in enumerate(unique_phrases)}\n",
    "\n",
    "    lose_facts = []\n",
    "\n",
    "    for triples in triple_tuples:\n",
    "        lose_facts.extend([tuple(t) for t in triples])\n",
    "\n",
    "    lose_fact_dict = {f: i for i, f in enumerate(lose_facts)}\n",
    "    fact_json = [{'idx': i, 'head': t[0], 'relation': t[1], 'tail': t[2]} for i, t in enumerate(lose_facts)]\n",
    "\n",
    "# passage mapping\n",
    "\n",
    "import json\n",
    "\n",
    "# Load data from output_mapping.json\n",
    "try:\n",
    "    with open('output_mapping.json', 'r') as f:\n",
    "        input_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: output_mapping.json file not found!\")\n",
    "    exit(1)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: output_mapping.json is not a valid JSON file!\")\n",
    "    exit(1)\n",
    "\n",
    "# Create a mapping of unique passages to passage IDs\n",
    "unique_passages = {}\n",
    "passage_id_to_original = {}\n",
    "passage_counter = 1\n",
    "\n",
    "for triple, passage in input_data.items():\n",
    "    if passage not in unique_passages:\n",
    "        passage_id = f\"passage{passage_counter}\"\n",
    "        unique_passages[passage] = passage_id\n",
    "        passage_id_to_original[passage_id] = passage\n",
    "        passage_counter += 1\n",
    "\n",
    "# Create triple to passage ID mapping\n",
    "triple_to_passage_id = {}\n",
    "for triple, passage in input_data.items():\n",
    "    passage_id = unique_passages[passage]\n",
    "    triple_to_passage_id[triple] = passage_id\n",
    "\n",
    "# Write the output files\n",
    "with open('triple_to_passage_id.json', 'w') as f:\n",
    "    json.dump(triple_to_passage_id, f, indent=2)\n",
    "\n",
    "with open('passage_id_to_original.json', 'w') as f:\n",
    "    json.dump(passage_id_to_original, f, indent=2)\n",
    "\n",
    "print(\"Files created successfully!\")\n",
    "print(f\"Created mappings for {len(triple_to_passage_id)} triples and {len(passage_id_to_original)} unique passages.\")\n",
    "\n",
    "# entity to passage mapping\n",
    "import json\n",
    "\n",
    "# Load data from output_mapping.json\n",
    "try:\n",
    "    with open('output_mapping.json', 'r') as f:\n",
    "        input_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: output_mapping.json file not found!\")\n",
    "    exit(1)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Error: output_mapping.json is not a valid JSON file!\")\n",
    "    exit(1)\n",
    "\n",
    "# Create a mapping of unique passages to passage IDs\n",
    "unique_passages = {}\n",
    "passage_id_to_original = {}\n",
    "passage_counter = 1\n",
    "\n",
    "for triple_str, passage in input_data.items():\n",
    "    if passage not in unique_passages:\n",
    "        passage_id = f\"passage{passage_counter}\"\n",
    "        unique_passages[passage] = passage_id\n",
    "        passage_id_to_original[passage_id] = passage\n",
    "        passage_counter += 1\n",
    "\n",
    "# Create entity to passage ID mapping (for head and tail only)\n",
    "entity_to_passage_ids = {}\n",
    "for triple_str, passage in input_data.items():\n",
    "    # Remove the parentheses and split the triple string\n",
    "    triple_parts = triple_str[1:-1].split(',')\n",
    "    if len(triple_parts) >= 2:  # Ensure at least head and tail exist\n",
    "        head = triple_parts[0].strip().strip(\"'\")\n",
    "        tail = triple_parts[-1].strip().strip(\"'\")  # Consider the last part as tail\n",
    "        passage_id = unique_passages[passage]\n",
    "\n",
    "        # Map head entity to passage ID\n",
    "        if head not in entity_to_passage_ids:\n",
    "            entity_to_passage_ids[head] = set()\n",
    "        entity_to_passage_ids[head].add(passage_id)\n",
    "\n",
    "        # Map tail entity to passage ID\n",
    "        if tail not in entity_to_passage_ids:\n",
    "            entity_to_passage_ids[tail] = set()\n",
    "        entity_to_passage_ids[tail].add(passage_id)\n",
    "    else:\n",
    "        print(f\"Warning: Skipping triple with insufficient parts: {triple_str}\")\n",
    "\n",
    "# Convert sets to lists for JSON serialization\n",
    "entity_to_passage_ids_list = {\n",
    "    entity: sorted(list(passage_ids)) for entity, passage_ids in entity_to_passage_ids.items()\n",
    "}\n",
    "\n",
    "# Write the output files\n",
    "with open('entity_to_passage_id.json', 'w') as f:\n",
    "    json.dump(entity_to_passage_ids_list, f, indent=2)\n",
    "\n",
    "with open('passage_id_to_original.json', 'w') as f:\n",
    "    json.dump(passage_id_to_original, f, indent=2)\n",
    "\n",
    "print(\"Files created successfully!\")\n",
    "print(f\"Created mappings for {len(entity_to_passage_ids)} unique entities (head/tail) and {len(passage_id_to_original)} unique passages.\")\n",
    "\n",
    "\n",
    "import json\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "# Load the entity to passage ID mapping\n",
    "with open('entity_to_passage_id.json', 'r') as f:\n",
    "    entity_to_passage_ids = json.load(f)\n",
    "\n",
    "# Your existing triples\n",
    "triples = fact_json\n",
    "\n",
    "# Create a directed graph\n",
    "kg = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges with metadata\n",
    "for triple in triples:\n",
    "    head = triple[\"head\"]\n",
    "    relation = triple[\"relation\"]\n",
    "    tail = triple[\"tail\"]\n",
    "    \n",
    "    # Get passage IDs for head and tail entities\n",
    "    head_passage_ids = entity_to_passage_ids.get(head, [])\n",
    "    tail_passage_ids = entity_to_passage_ids.get(tail, [])\n",
    "    \n",
    "    # Add nodes with passage ID metadata\n",
    "    kg.add_node(head, passage_ids=head_passage_ids)\n",
    "    kg.add_node(tail, passage_ids=tail_passage_ids)\n",
    "    \n",
    "    # Add edge with relation metadata\n",
    "    kg.add_edge(head, tail, relation=relation)\n",
    "\n",
    "print(f\"ðŸ“Œ Knowledge Graph Created: {kg.number_of_nodes()} Nodes, {kg.number_of_edges()} Edges\")\n",
    "\n",
    "# Example of accessing node metadata\n",
    "\n",
    "random_node = random.choice(list(kg.nodes()))\n",
    "print(f\"Example node '{random_node}' has these passage IDs: {kg.nodes[random_node]['passage_ids']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69578911",
   "metadata": {},
   "source": [
    "# Q/A generation via KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usage python multihop_qa_generator.py --input your_input_file.jsonl --output multihop_results.json\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class MistralWrapper:\n",
    "    def __init__(self, model_id=\"mistralai/Mistral-7B-Instruct-v0.2\", api_key=None, temperature=0.1, max_tokens=1024):\n",
    "        \"\"\"\n",
    "        Initialize the Mistral model wrapper.\n",
    "        \n",
    "        Args:\n",
    "            model_id: The Hugging Face model ID\n",
    "            api_key: Hugging Face API key (if None, loads from HF_API_KEY env variable)\n",
    "            temperature: Temperature for text generation\n",
    "            max_tokens: Maximum number of tokens to generate\n",
    "        \"\"\"\n",
    "        self.model_id = model_id\n",
    "        self.api_key = api_key if api_key else os.getenv(\"HF_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"API key not provided. Set HF_API_KEY environment variable or pass api_key parameter.\")\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "\n",
    "    def __call__(self, prompt):\n",
    "        \"\"\"\n",
    "        Generate a response using the Mistral model.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The input prompt\n",
    "            \n",
    "        Returns:\n",
    "            The generated text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format the prompt according to Mistral's chat template\n",
    "            formatted_prompt = f\"\"\"<s>[INST] {prompt} [/INST]\"\"\"\n",
    "            \n",
    "            payload = {\n",
    "                \"inputs\": formatted_prompt,\n",
    "                \"parameters\": {\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"max_new_tokens\": self.max_tokens,\n",
    "                    \"return_full_text\": False,\n",
    "                    \"do_sample\": True,\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            return response.json()[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Hugging Face API: {e}\")\n",
    "            return None\n",
    "\n",
    "def parse_json_document(json_line):\n",
    "    \"\"\"\n",
    "    Parse the JSON document and extract facts and passages.\n",
    "    \n",
    "    Args:\n",
    "        json_line: A line from the input file containing JSON data\n",
    "        \n",
    "    Returns:\n",
    "        A tuple of (facts, passages, title)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(json_line)\n",
    "        document = data.get(\"document\", \"\")\n",
    "        title = data.get(\"chapter_title\", \"\")\n",
    "        \n",
    "        # Extract facts\n",
    "        facts_match = re.search(r'<facts>(.*?)<\\/facts>', document, re.DOTALL)\n",
    "        facts = []\n",
    "        if facts_match:\n",
    "            facts_text = facts_match.group(1)\n",
    "            fact_matches = re.findall(r'<fact>(.*?)<\\/fact>', facts_text, re.DOTALL)\n",
    "            facts = [fact.strip() for fact in fact_matches]\n",
    "        \n",
    "        # Extract passages\n",
    "        passages_match = re.search(r'<passages>(.*?)<\\/passages>', document, re.DOTALL)\n",
    "        passages = []\n",
    "        if passages_match:\n",
    "            passages_text = passages_match.group(1)\n",
    "            passage_matches = re.findall(r'<passage>(.*?)<\\/passage>', passages_text, re.DOTALL)\n",
    "            passages = [passage.strip() for passage in passage_matches]\n",
    "        \n",
    "        return facts, passages, title\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing JSON line: {json_line[:100]}...\")\n",
    "        return [], [], \"\"\n",
    "\n",
    "def create_multihop_prompt(facts, passages):\n",
    "    \"\"\"\n",
    "    Create a prompt for generating multi-hop questions based on facts and passages.\n",
    "    \n",
    "    Args:\n",
    "        facts: List of facts\n",
    "        passages: List of passages\n",
    "        \n",
    "    Returns:\n",
    "        A formatted prompt string\n",
    "    \"\"\"\n",
    "    facts_text = \"\\n\".join([f\"<fact>{fact}</fact>\" for fact in facts])\n",
    "    passages_text = \"\\n\".join([f\"<passage>{passage}</passage>\" for passage in passages])\n",
    "    \n",
    "    prompt = f\"\"\"You are a Multi-Hop Factual Question Formulation Assistant. Generate precise, fact-based, multi-hop questions requiring integration of specific information from provided texts, resulting in very brief factoid answers (2-3 words maximum).\n",
    "\n",
    "Given the following facts and passages, please generate multi-hop questions:\n",
    "\n",
    "Facts:\n",
    "{facts_text}\n",
    "\n",
    "Passages:\n",
    "{passages_text}\n",
    "\n",
    "Generate 1-2 multi-hop questions that require integrating multiple pieces of information from these texts.\n",
    "Each question should have a clear factoid answer (2-3 words maximum).\n",
    "Begin with subquestions that build towards a final complex question.\n",
    "Please follow this exact format:\n",
    "\n",
    "<sub-question>[First focused question]</sub-question>\n",
    "<answer-to-sub-question>[Concise answer]</answer-to-sub-question>\n",
    "\n",
    "<sub-question>[Follow-up question using previous answer]</sub-question>\n",
    "<answer-to-sub-question>[Concise answer]</answer-to-sub-question>\n",
    "\n",
    "[Additional sub-questions as needed]\n",
    "\n",
    "<question-type>[Number of reasoning steps, e.g., \"2-hop\"]</question-type>\n",
    "\n",
    "<complex-question>[Question that integrates all previous sub-questions]</complex-question>\n",
    "<answer-to-complex-question>[Final answer, 2-3 words]</answer-to-complex-question>\n",
    "\n",
    "<explanation>[Break down how to answer the complex question step-by-step]</explanation>\n",
    "<done>\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def extract_qa_from_response(response):\n",
    "    \"\"\"\n",
    "    Extract the multi-hop questions, answers, and explanations from the model's response.\n",
    "    \n",
    "    Args:\n",
    "        response: The model's generated text\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary containing the extracted QA components\n",
    "    \"\"\"\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    # Extract components using regex\n",
    "    qa_dict = {\n",
    "        \"sub_questions\": [],\n",
    "        \"sub_answers\": [],\n",
    "        \"question_type\": None,\n",
    "        \"complex_question\": None,\n",
    "        \"complex_answer\": None,\n",
    "        \"explanation\": None\n",
    "    }\n",
    "    \n",
    "    # Extract sub-questions and their answers\n",
    "    sub_questions = re.findall(r'<sub-question>(.*?)<\\/sub-question>', response, re.DOTALL)\n",
    "    sub_answers = re.findall(r'<answer-to-sub-question>(.*?)<\\/answer-to-sub-question>', response, re.DOTALL)\n",
    "    \n",
    "    for i in range(min(len(sub_questions), len(sub_answers))):\n",
    "        qa_dict[\"sub_questions\"].append(sub_questions[i].strip())\n",
    "        qa_dict[\"sub_answers\"].append(sub_answers[i].strip())\n",
    "    \n",
    "    # Extract question type\n",
    "    question_type_match = re.search(r'<question-type>(.*?)<\\/question-type>', response, re.DOTALL)\n",
    "    if question_type_match:\n",
    "        qa_dict[\"question_type\"] = question_type_match.group(1).strip()\n",
    "    \n",
    "    # Extract complex question and answer\n",
    "    complex_q_match = re.search(r'<complex-question>(.*?)<\\/complex-question>', response, re.DOTALL)\n",
    "    if complex_q_match:\n",
    "        qa_dict[\"complex_question\"] = complex_q_match.group(1).strip()\n",
    "    \n",
    "    complex_a_match = re.search(r'<answer-to-complex-question>(.*?)<\\/answer-to-complex-question>', response, re.DOTALL)\n",
    "    if complex_a_match:\n",
    "        qa_dict[\"complex_answer\"] = complex_a_match.group(1).strip()\n",
    "    \n",
    "    # Extract explanation\n",
    "    explanation_match = re.search(r'<explanation>(.*?)<\\/explanation>', response, re.DOTALL)\n",
    "    if explanation_match:\n",
    "        qa_dict[\"explanation\"] = explanation_match.group(1).strip()\n",
    "    \n",
    "    return qa_dict\n",
    "\n",
    "def process_jsonl_file(input_file, output_file, model):\n",
    "    \"\"\"\n",
    "    Process a JSONL file containing documents and generate multi-hop questions.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input JSONL file\n",
    "        output_file: Path to save the output JSON results\n",
    "        model: The language model wrapper\n",
    "        \n",
    "    Returns:\n",
    "        A list of results containing generated multi-hop QAs\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line_idx, line in enumerate(tqdm(lines, desc=\"Processing documents\")):\n",
    "        try:\n",
    "            facts, passages, title = parse_json_document(line)\n",
    "            \n",
    "            if not facts or not passages:\n",
    "                print(f\"Skipping document {line_idx} - missing facts or passages\")\n",
    "                continue\n",
    "            \n",
    "            prompt = create_multihop_prompt(facts, passages)\n",
    "            response = model(prompt)\n",
    "            \n",
    "            qa_data = extract_qa_from_response(response)\n",
    "            \n",
    "            if qa_data:\n",
    "                result = {\n",
    "                    \"title\": title,\n",
    "                    \"facts\": facts,\n",
    "                    \"passages\": passages,\n",
    "                    \"qa_data\": qa_data\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"Generated QA for document {line_idx} - {title}\")\n",
    "            else:\n",
    "                print(f\"Failed to extract QA from response for document {line_idx}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {line_idx}: {e}\")\n",
    "    \n",
    "    # Save the results to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_text_data(text_data, output_file=None):\n",
    "    \"\"\"\n",
    "    Process a string containing multiple JSON documents and generate multi-hop questions.\n",
    "    \n",
    "    Args:\n",
    "        text_data: String containing JSON documents (one per line)\n",
    "        output_file: Optional path to save the output JSON results\n",
    "        \n",
    "    Returns:\n",
    "        A list of results containing generated multi-hop QAs\n",
    "    \"\"\"\n",
    "    # Initialize the Mistral model\n",
    "    model = MistralWrapper(model_id=\"mistralai/Mixtral-8x22B-Instruct-v0.1\")\n",
    "    \n",
    "    results = []\n",
    "    lines = text_data.strip().split('\\n')\n",
    "    \n",
    "    for line_idx, line in enumerate(tqdm(lines, desc=\"Processing documents\")):\n",
    "        try:\n",
    "            facts, passages, title = parse_json_document(line)\n",
    "            \n",
    "            if not facts or not passages:\n",
    "                print(f\"Skipping document {line_idx} - missing facts or passages\")\n",
    "                continue\n",
    "            \n",
    "            prompt = create_multihop_prompt(facts, passages)\n",
    "            response = model(prompt)\n",
    "            \n",
    "            qa_data = extract_qa_from_response(response)\n",
    "            \n",
    "            if qa_data:\n",
    "                result = {\n",
    "                    \"title\": title,\n",
    "                    \"facts\": facts,\n",
    "                    \"passages\": passages,\n",
    "                    \"qa_data\": qa_data\n",
    "                }\n",
    "                results.append(result)\n",
    "                print(f\"Generated QA for document {line_idx} - {title}\")\n",
    "            else:\n",
    "                print(f\"Failed to extract QA from response for document {line_idx}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {line_idx}: {e}\")\n",
    "    \n",
    "    # Save the results to the output file if specified\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Parse command line arguments\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Generate multi-hop QA pairs from documents')\n",
    "    parser.add_argument('--input', type=str, help='Input JSONL file path')\n",
    "    parser.add_argument('--output', type=str, default='multihop_qa_results.json', help='Output JSON file path')\n",
    "    parser.add_argument('--model', type=str, default='mistralai/Mixtral-8x22B-Instruct-v0.1', help='Hugging Face model ID')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize the language model\n",
    "    try:\n",
    "        model = MistralWrapper(model_id=args.model)\n",
    "        print(f\"Using model: {args.model}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error initializing model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Process the input file\n",
    "    results = process_jsonl_file(args.input, args.output, model)\n",
    "    print(f\"Processed {len(results)} documents. Results saved to {args.output}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb7f73",
   "metadata": {},
   "source": [
    "# Path Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "\n",
    "with open(\"mappings/passage_id_to_passage.json\", \"r\") as f:\n",
    "    passage_id_to_text = json.load(f)\n",
    "\n",
    "def sample_paths_with_passages(kg, num_paths=50, min_hops=3, max_hops=5, min_passages=2, max_passages=5):\n",
    "    paths = []\n",
    "    attempts = 0\n",
    "    max_attempts = num_paths * 20\n",
    "\n",
    "    nodes = list(kg.nodes())\n",
    "\n",
    "    while len(paths) < num_paths and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        facts = []\n",
    "        visited_nodes = set()\n",
    "        visited_passages = set()\n",
    "\n",
    "        candidate_nodes = [n for n in nodes if len(kg.nodes[n]['passage_ids']) >= min_passages]\n",
    "        if not candidate_nodes:\n",
    "            print(\"No suitable starting nodes found. Lower 'min_passages'.\")\n",
    "            break\n",
    "\n",
    "        current_node = random.choice(candidate_nodes)\n",
    "        current_passages = set(kg.nodes[current_node]['passage_ids'])\n",
    "        if len(current_passages) > max_passages:\n",
    "            current_passages = set(random.sample(list(current_passages), max_passages))\n",
    "\n",
    "        visited_nodes.add(current_node)\n",
    "        visited_passages.update(current_passages)\n",
    "\n",
    "        hop_count = random.randint(min_hops, max_hops)\n",
    "        hop_counter = 0\n",
    "\n",
    "        while hop_counter < hop_count:\n",
    "            neighbors = []\n",
    "            for neighbor in kg.neighbors(current_node):\n",
    "                if neighbor in visited_nodes:\n",
    "                    continue\n",
    "                neighbor_passages = set(kg.nodes[neighbor]['passage_ids'])\n",
    "                new_passages = neighbor_passages - visited_passages\n",
    "\n",
    "                if len(visited_passages | new_passages) <= max_passages:\n",
    "                    relation = kg.edges[current_node, neighbor]['relation']\n",
    "                    fact_str = f\"{current_node} {relation} {neighbor}\"\n",
    "                    neighbors.append((neighbor, fact_str, new_passages))\n",
    "\n",
    "            if not neighbors:\n",
    "                break\n",
    "\n",
    "            neighbors.sort(key=lambda x: len(x[2]), reverse=True)\n",
    "            top_choices = neighbors[:3]\n",
    "            chosen_neighbor, fact_str, passages_added = random.choice(top_choices)\n",
    "\n",
    "            facts.append(fact_str)\n",
    "            visited_nodes.add(chosen_neighbor)\n",
    "            visited_passages.update(passages_added)\n",
    "            current_node = chosen_neighbor\n",
    "            hop_counter += 1\n",
    "\n",
    "            if len(visited_passages) == max_passages:\n",
    "                break\n",
    "\n",
    "        if len(facts) >= min_hops and len(visited_passages) >= min_passages:\n",
    "            # Convert passage IDs to actual passage texts\n",
    "            text_passages = [\n",
    "                passage_id_to_text[str(pid)]\n",
    "                for pid in visited_passages\n",
    "                if str(pid) in passage_id_to_text\n",
    "            ]\n",
    "            paths.append({\n",
    "                \"facts\": facts,\n",
    "                \"passages\": text_passages\n",
    "            })\n",
    "\n",
    "    print(f\"âœ… Sampled {len(paths)} paths (after {attempts} attempts).\")\n",
    "    return paths\n",
    "\n",
    "# Generate the data\n",
    "sampled_data = sample_paths_with_passages(\n",
    "    kg,\n",
    "    num_paths=10000,\n",
    "    min_hops=3,\n",
    "    max_hops=5,\n",
    "    min_passages=2,\n",
    "    max_passages=5\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "with open(\"sampled_paths_with_text_passages.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sampled_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"ðŸ“„ Saved to 'sampled_paths_with_text_passages.json'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc8daf9",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f663af",
   "metadata": {},
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "def load_qa_data(jsonl_path):\n",
    "    data = []\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            q = item[\"question\"].strip()\n",
    "            a = item[\"answer\"].strip()\n",
    "            full_text = f\"<s>[INST] Answer the following question:\\n\\n{q}\\n[/INST]\\n{a}</s>\"\n",
    "            data.append({\"text\": full_text})\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "def tokenize_fn(examples, tokenizer, max_length=2048):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "\n",
    "def main():\n",
    "    model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "    jsonl_path = \"qa_data.jsonl\"\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # LoRA Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Load and tokenize dataset\n",
    "    dataset = load_qa_data(jsonl_path)\n",
    "    dataset = dataset.train_test_split(test_size=0.05)\n",
    "    tokenized = dataset.map(lambda x: tokenize_fn(x, tokenizer), batched=True)\n",
    "    tokenized = tokenized.remove_columns([\"text\"])\n",
    "\n",
    "    # Training setup\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints-mistral-qa\",\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=4,\n",
    "        fp16=True,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"test\"],\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    )\n",
    "\n",
    "    # Train & Save\n",
    "    trainer.train()\n",
    "    model.save_pretrained(\"./mistral-qa-final\")\n",
    "    tokenizer.save_pretrained(\"./mistral-qa-final\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
